# Grok Review - Action Plan for v0.3

**Date**: November 22, 2025  
**Reviewer**: Grok  
**Current Score**: 8.8 / 10  
**Target Score**: 9.7 / 10 (arXiv-ready)

---

## üéØ **Grok's Overall Assessment**

> "Unusually strong rough draft... already at the level of many accepted workshop papers... With 2-3 days of focused polishing, this can become a genuinely high-impact arXiv paper."

**Key Strength**: Core claim is provocative, timely, and well-supported.

---

## üö® **HIGH-PRIORITY FIXES (Must Do Before Submission)**

### 1. ‚úÖ **ALREADY FIXED: SmolLM2-360M Numbers**
- **Status**: DONE in v0.2
- **What we did**: Replaced all estimated values with actual results from CAPACITY_SCALING_ANALYSIS.md
- **Current values**: 11.4% JSONExact, 0.240 Field F1, 23.7% Compliance
- **No remaining "~" or asterisks** ‚úÖ

### 2. ‚ö†Ô∏è **CRITICAL: Qwen2.5-0.5B Latency (9,480 ms) - INVESTIGATE**
- **Issue**: 9.5 seconds seems 2-3√ó too slow for i9 CPU
- **Possible causes**:
  - Wrong model loaded (7B instead of 0.5B?)
  - Including tokenization time?
  - CPU-only with no optimization?
- **Action**: 
  - Verify we tested the correct model (Qwen/Qwen2.5-0.5B)
  - Re-check evaluation logs
  - Option 1: Re-run with timing breakdown
  - Option 2: Remove latency column from tables (safer)
- **RECOMMENDATION**: Remove latency from tables for v0.3 (it's not core to the claim)

### 3. ‚ö†Ô∏è **Citation Years - VERIFY AND FIX**
- **SmolLM2**: Listed as 2024? (verify actual release date)
- **SLM-Bench (Pham et al.)**: Listed as 2025 (verify if actually published)
- **Action**: Check actual publication dates and update citations

### 4. ‚ö†Ô∏è **Tone Down "Breaks Zero Wall" Language**
- **Current**: "first model to break the zero wall ‚Äî a qualitative capability shift"
- **Issue**: 4% is scientifically interesting but practically useless
- **Suggested**: "the first sub-500M model to achieve non-zero exact-match performance on complex schemas (4.0%), revealing an abrupt capacity transition"
- **Action**: Apply this rewording in v0.3

### 5. ‚ö†Ô∏è **Add Train/Test No-Overlap Statement**
- **Issue**: Risk of perceived data leakage (same 24 schemas, 80/20 split)
- **Action**: Add explicit statement in ¬ß3.2 or ¬ß5.1:
  > "The train/test split is stratified by schema and complexity level; no test example or schema appears in the training set, eliminating contamination risk."

---

## üìã **MEDIUM-PRIORITY (Strongly Recommended)**

### 1. ‚úÖ **Abstract Length - NEEDS TRIMMING**
- **Current**: ~228 words
- **Target**: ‚â§180 words for arXiv
- **Action**: Remove detailed numbers, keep only headline results
- **Priority**: HIGH (easy win)

### 2. ‚è≥ **Add One Stronger Zero-Shot Baseline**
- **Issue**: Only one <1B comparison (Qwen2.5-0.5B zero-shot, performs poorly)
- **Options**:
  - Qwen2.5-0.5B-Instruct (instruction-tuned version)
  - Llama-3.2-1B (zero-shot)
  - MobileLLaMA-1.4B
- **Action**: Consider adding one more baseline (time permitting)
- **Note**: This strengthens the claim but not critical

### 3. ‚è≥ **Replace ASCII Figures with Real Plots**
- **Current**: ASCII art figures (functional but unprofessional)
- **Action**: Create matplotlib/seaborn plots (PDF format)
- **Priority**: MEDIUM (reviewers love clean plots)
- **Time**: ~30-60 minutes

### 4. ü§î **Taxonomy (NLM/MLM/SLM/LLM) - CONSIDER DROPPING**
- **Issue**: Boundaries feel arbitrary, not used consistently in paper
- **Options**:
  - Drop formal taxonomy
  - Justify with more evidence
  - De-emphasize (mention once, don't structure paper around it)
- **RECOMMENDATION**: De-emphasize but keep (already in paper)

### 5. ‚è≥ **Related Work - Add 2025 Models**
- **Missing**:
  - TinyChat (2025)
  - H2O-Danube3 (2025)
  - Microsoft Phi-3.5-mini-Instruct (Aug 2025)
- **Action**: Add 3-5 citations to strengthen Related Work

---

## üé® **LOW-PRIORITY / POLISH**

### 1. Title Refinement
- **Current**: "Task-Specialized Micro Language Models Outperform Larger Zero-Shot Models on Structured Data Extraction"
- **Suggested**: "Task-Specialized Micro Models Outperform Larger Generalists on Structured Data Extraction for Edge Deployment"
- **Assessment**: Current title is fine, suggestion is marginally better

### 2. Introduction Hook
- **Suggestion**: Move "13√ó improvement in <2 minutes" earlier (it's the killer hook)
- **Action**: Consider restructuring opening bullets

### 3. ¬ß3.4 Metrics - Clarify Field F1
- **Add**: One-line intuition for why Field F1 is macro-averaged

### 4. ¬ß5.7 Error Analysis
- **Current**: Markdown table
- **Suggested**: LaTeX table for polish

### 5. Conclusion - Pre-empt "Why not 50M?" Question
- **Add**: Sentence comparing to BitNet b1.58 or Ternary ultra-small models

---

## üìä **PRIORITY RANKING FOR v0.3**

### Must Do (Before arXiv Submission)
1. ‚úÖ **DONE**: Fix SmolLM2-360M numbers
2. ‚ö†Ô∏è **ACTION REQUIRED**: Investigate/fix Qwen latency OR remove latency column
3. ‚ö†Ô∏è **ACTION REQUIRED**: Verify and fix citation years
4. ‚ö†Ô∏è **ACTION REQUIRED**: Tone down "breaks zero wall" language
5. ‚ö†Ô∏è **ACTION REQUIRED**: Add train/test no-overlap statement
6. ‚úÖ **ACTION REQUIRED**: Trim abstract to ‚â§180 words

### Should Do (Strongly Recommended)
7. ‚è≥ **CONSIDER**: Add one more zero-shot baseline
8. ‚è≥ **NICE TO HAVE**: Replace ASCII figures with real plots
9. ‚è≥ **NICE TO HAVE**: Add 2025 model citations to Related Work

### Nice to Have (Polish)
10. üé® **OPTIONAL**: Refine title
11. üé® **OPTIONAL**: Restructure intro hook
12. üé® **OPTIONAL**: Polish tables and metrics explanations

---

## üéØ **RECOMMENDED WORKFLOW**

### Day 1 (Today) - HIGH PRIORITY ONLY
1. ‚úÖ Check if Qwen latency is accurate (review logs)
2. ‚úÖ Decision: Keep latency with explanation OR remove column
3. ‚úÖ Verify citation years (SmolLM2, SLM-Bench)
4. ‚úÖ Apply "breaks zero wall" rewording
5. ‚úÖ Add train/test stratification statement
6. ‚úÖ Trim abstract to ‚â§180 words
7. ‚úÖ Save as v0.3

### Day 2 (Optional) - MEDIUM PRIORITY
1. ‚è≥ Create real plots (matplotlib) to replace ASCII
2. ‚è≥ Add 3-5 citations to Related Work (2025 models)
3. ‚è≥ Consider adding one more baseline (if time)

### Day 3 (Final Polish) - LOW PRIORITY
1. üé® Final proofread
2. üé® Convert to LaTeX/PDF
3. üé® Submit to arXiv

---

## üìù **SPECIFIC EDITS FOR v0.3**

### Edit 1: Qwen Latency - DECISION NEEDED

**Option A (Remove latency column):**
```markdown
| Model | Params | JSONExact | Field F1 | Compliance | Size |
```

**Option B (Keep with caveat):**
```markdown
| Model | Params | JSONExact | Field F1 | Compliance | Size | Latency* |
...
* CPU-only evaluation on Intel i9; times include full inference pipeline
```

**RECOMMENDATION**: Option A (remove latency - not core to claim)

### Edit 2: "Breaks Zero Wall" Rewording

**Current:**
> "Maaza-SLM-360M is the first model to break the 'zero wall' on complex schemas, achieving 4.0% JSONExact. While low in absolute terms, this represents a qualitative capability shift not seen in smaller models or comparable zero-shot models."

**Revised:**
> "Maaza-SLM-360M is the first sub-500M model to achieve non-zero exact-match performance on complex schemas (4.0%), revealing an abrupt capacity transition around 300M parameters. While low in absolute terms, this result demonstrates a qualitative capability boundary not observed in smaller models or larger zero-shot baselines."

### Edit 3: Train/Test Stratification Statement

**Add to ¬ß3.2 (Dataset Construction) or ¬ß5.1 (Experimental Setup):**
> "The train/test split is stratified by schema type and complexity level, ensuring proportional representation across all 24 schemas. No test example or schema variant appears in the training set, eliminating data contamination risk."

### Edit 4: Abstract Trimming (228 ‚Üí ‚â§180 words)

**Remove**:
- Detailed parameter counts in middle section
- Specific "3.7√ó smaller" numbers (keep in intro)

**Keep**:
- Core finding
- Headline numbers (24.7%, 55.1%)
- Open-source statement

---

## ‚úÖ **QUESTIONS TO RESOLVE**

1. **Qwen Latency**: Is 9.5s accurate? Should we remove latency column?
2. **SmolLM2 Year**: 2024 or 2025? (need to verify)
3. **SLM-Bench**: Is Pham et al. 2025 published or preprint?
4. **Additional Baseline**: Do we have time to run Qwen2.5-0.5B-Instruct or Llama-3.2-1B?
5. **Figures**: Convert ASCII to matplotlib now or defer to post-arXiv?

---

## üì¶ **DELIVERABLES**

After addressing high-priority items:
1. `MAAZA_PAPER_V03_ARXIV_READY.md` (revised draft)
2. `REVISION_LOG_V02_TO_V03.md` (changelog)
3. `GROK_REVIEW_RESPONSE.md` (this file + how we addressed each point)

---

**Grok's Final Score Prediction**: 9.7 / 10 (after fixes)

**Our Assessment**: Grok is right. These are smart, actionable fixes. Priority is HIGH-PRIORITY items (1-6), then decide on medium-priority based on time.

---

**Status**: ‚è≥ Action plan created, awaiting decisions on:
- Latency column (remove or keep?)
- Additional baseline (worth the time?)
- Citation years (need to verify)

**Next**: Address high-priority items for v0.3

