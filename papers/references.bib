% BibTeX References for Maaza Paper
% Generated: November 21, 2025
% Source: GPT collaboration + manual curation

% ========================================
% Foundational Transformer Papers
% ========================================

@article{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={NAACL-HLT},
  year={2019}
}

@inproceedings{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

% ========================================
% Small Language Models (SLMs)
% ========================================

@article{allal2024smollm2,
  title={SmolLM2: When Smol Goes Big -- Data-Centric Training of Small Language Models},
  author={Allal, Louis B and Wolf, Thomas and others},
  journal={arXiv preprint arXiv:2502.02737},
  year={2024}
}

@article{zhang2024tinyllama,
  title={TinyLlama: An Open-Source Small Language Model},
  author={Zhang, Jin and Xu, Shuohang and others},
  journal={arXiv preprint arXiv:2401.02385},
  year={2024}
}

@article{yang2024qwen25,
  title={Qwen2.5 Technical Report},
  author={Yang, An and Yang, Baosong and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{hui2024qwen25coder,
  title={Qwen2.5-Coder: A Family of Open Code LLMs},
  author={Hui, Bingsheng and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@article{abdin2024phi3,
  title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  author={Abdin, Mahmoud and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{gemma2024gemma2,
  title={Gemma 2: Improving Open Language Models at a Practical Size},
  author={{Gemma Team}},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@misc{meta2024llama32,
  title={Llama 3.2: Revolutionizing Edge AI and Vision with Open Models},
  author={{Meta AI}},
  howpublished={Meta AI Blog},
  year={2024},
  url={https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/}
}

% ========================================
% Distillation & Compression
% ========================================

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS Workshop on Energy Efficient Machine Learning and Cognitive Computing},
  year={2019}
}

@inproceedings{jiao2020tinybert,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle={Findings of EMNLP},
  year={2020}
}

@inproceedings{sun2020mobilebert,
  title={MobileBERT: A Compact Task-Agnostic BERT for Resource-Limited Devices},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  booktitle={Proceedings of ACL},
  year={2020}
}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={NeurIPS Deep Learning Workshop},
  year={2015}
}

@article{han2015deepcompression,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={ICLR},
  year={2016}
}

@article{jacob2018quantization,
  title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and others},
  journal={CVPR},
  year={2018}
}

% ========================================
% Parameter-Efficient Fine-Tuning
% ========================================

@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={NeurIPS},
  year={2023}
}

% ========================================
% Benchmarks
% ========================================

@article{pham2025slmbench,
  title={SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental and Efficiency Impacts},
  author={Pham, Nguyen Thai and others},
  journal={Findings of EMNLP},
  year={2025}
}

@article{hendrycks2021mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and others},
  journal={ICLR},
  year={2021}
}

@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={ACL},
  year={2019}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mehran and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={ICLR},
  year={2018}
}

@article{wang2019superglue,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and others},
  journal={NeurIPS},
  year={2019}
}

% ========================================
% Edge AI & Deployment
% ========================================

@article{zheng2024reviewedgeLLM,
  title={A Review on Edge Large Language Models: Design, Execution, and Applications},
  author={Zheng, Yue and Chen, Yuhao and Qian, Bin and Shi, Xiufang and Shu, Yuanchao and Chen, Jiming},
  journal={arXiv preprint arXiv:2410.11845},
  year={2024}
}

@article{wang2025edgeefficient,
  title={A Survey of Edge Efficient LLMs and Techniques},
  author={Wang, Rui and others},
  journal={Journal of Systems Architecture},
  year={2025}
}

@article{zeng2024webllm,
  title={WebLLM: A High-Performance In-Browser LLM Inference Engine},
  author={Zeng, Yanqi and Chen, Tianqi and others},
  journal={arXiv preprint arXiv:2412.15803},
  year={2024}
}

@misc{hf2024transformersjs,
  title={Transformers.js v3: WebGPU Support, New Models \& Demos},
  author={{Hugging Face}},
  howpublished={Hugging Face Blog},
  year={2024},
  url={https://huggingface.co/blog/transformersjs-v3}
}

@misc{microsoft2025edgeapis,
  title={Opening On-Device AI Models to Web Apps in Edge},
  author={{Microsoft Edge Team}},
  howpublished={Microsoft Edge Blog},
  year={2025},
  url={https://www.theverge.com/news/669528/microsoft-ai-edge-browser-web-app-build-apis}
}

@misc{tensorflowlite2017,
  title={TensorFlow Lite},
  author={{TensorFlow Team}},
  howpublished={https://www.tensorflow.org/lite},
  year={2017}
}

@misc{onnxruntime2018,
  title={ONNX Runtime: High-Performance Inference Engine},
  author={{ONNX Runtime Team}},
  howpublished={https://onnxruntime.ai},
  year={2018}
}

% ========================================
% Our Work (Placeholders)
% ========================================

@article{maaza2025edgejson,
  title={Maaza: Task-Specialized Micro Language Models Outperform Larger Zero-Shot Models on Structured Data Extraction},
  author={{CycleCore Technologies}},
  journal={arXiv preprint},
  year={2025},
  note={In preparation}
}

@article{edgejson2025benchmark,
  title={EdgeJSON: A Benchmark for Structured JSON Extraction on Edge Devices},
  author={{CycleCore Technologies}},
  journal={arXiv preprint},
  year={2025},
  note={In preparation}
}

